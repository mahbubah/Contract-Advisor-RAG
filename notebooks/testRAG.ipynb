{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-11/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from chromadb import Client\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import openai\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    docx_texts = [paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
    "\n",
    "    character_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], chunk_size=500, chunk_overlap=0)\n",
    "    character_split_texts = character_splitter.split_text('\\n\\n'.join(docx_texts))\n",
    "\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "    token_split_texts = []\n",
    "    for text in character_split_texts:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "    return token_split_texts\n",
    "\n",
    "# Example document path\n",
    "docx_path = \"../Evaluation Sets/Raptor Contract.docx\"\n",
    "documents = preprocess_document(docx_path)\n",
    "\n",
    "# Embedding function\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "# Create ChromaDB collection and add documents\n",
    "chroma_collection = chroma_client.create_collection(\"Contract\", embedding_function=embedding_function)\n",
    "ids = [str(i) for i in range(len(documents))]\n",
    "chroma_collection.add(ids=ids, documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(query, collection):\n",
    "    results = collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results['documents'][0]\n",
    "    return retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\\n    information = \"\\n\\n\".join(retrieved_documents)\\n\\n    messages = [\\n        {\\n            \"role\": \"system\",\\n            \"content\": \"You are a helpful expert contract advisor assistant. Your users are asking questions about information contained in the contract.\"\\n                       \"You will be shown the user\\'s question, and the relevant information from the contract. Answer the user\\'s question using only this information.\"\\n        },\\n        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\\n    ]\\n    \\n    response = openai.ChatCompletion.create(\\n        model=model,\\n        messages=messages,\\n    )\\n    content = response.choices[0].message[\\'content\\']\\n    return content\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert contract advisor assistant. Your users are asking questions about information contained in the contract.\"\n",
    "                       \"You will be shown the user's question, and the relevant information from the contract. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message['content']\n",
    "    return content\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def rank_documents(query, documents):\\n    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\\n    pairs = [[query, doc] for doc in documents]\\n    scores = cross_encoder.predict(pairs)\\n    ranked_indices = np.argsort(scores)[::-1]\\n    ranked_documents = [documents[i] for i in ranked_indices]\\n    return ranked_documents\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def rank_documents(query, documents):\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "    ranked_documents = [documents[i] for i in ranked_indices]\n",
    "    return ranked_documents\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_multiple_query(query, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert contract advisor assistant. Your users are asking questions about information contained in the contract.\"\n",
    "                       \"Suggest up to five additional related questions to help them find the information they need, for the provided question. \"\n",
    "                       \"Suggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topic.\"\n",
    "                       \"Make sure they are complete questions, and that they are related to the original question.\"\n",
    "                       \"Output one question per line. Do not number the questions.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message['content']\n",
    "    content = content.split(\"\\n\")\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_expansion(original_query, model=\"gpt-3.5-turbo\"):\n",
    "    # Expand the original query\n",
    "    expanded_queries = augment_multiple_query(original_query, model=model)[:5]  # Limit to top 5 expanded queries\n",
    "\n",
    "    # Include the original query in the list of queries\n",
    "    queries = [original_query] + expanded_queries\n",
    "\n",
    "    # Query ChromaDB for relevant documents\n",
    "    results = chroma_collection.query(query_texts=queries, n_results=5, include=['documents'])\n",
    "    retrieved_documents = results['documents']\n",
    "\n",
    "    # Deduplicate the retrieved documents\n",
    "    unique_documents = set()\n",
    "    for documents in retrieved_documents:\n",
    "        for document in documents:\n",
    "            unique_documents.add(document)\n",
    "\n",
    "    # Convert back to list for indexing\n",
    "    unique_documents = list(unique_documents)\n",
    "\n",
    "    # Rank documents based on relevance to each query\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    pairs = [[query, doc] for doc in unique_documents for query in queries]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    # Sort documents based on average score across all queries\n",
    "    avg_scores = np.mean(np.array(scores).reshape(len(queries), -1), axis=0)\n",
    "    ranked_indices = np.argsort(avg_scores)[::-1]\n",
    "    ranked_documents = [unique_documents[i] for i in ranked_indices]\n",
    "\n",
    "    # Generate answer using RAG with ranked documents\n",
    "    information = \"\\n\\n\".join(ranked_documents[:5])  # Use top 5 ranked documents\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert legal contract advisor assistant. Your users are asking questions about information contained in the contract.\"\n",
    "                       \"You will be shown the user's question, and the relevant information from the contract. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {original_query}. \\n Information: {information}\"},\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Provide concise answers without additional explanation unless explicitly requested.\"\n",
    "                    \"Identify questions that require a simple yes or no response, and Provide concise answers without additional explanation unless explicitly requested.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message['content']\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "No, the escrow amount may not serve as a recourse for the Buyer in case of breach of representations by the Company. Recovery from the escrow amount shall constitute the buyer's exclusive remedy against the company securityholders in connection with any claim relating to any adjustments of the purchase price.\n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Example query\n",
    "    #query = \"What is the purpose of the escrow? \"\n",
    "    \n",
    "    original_query = \"May the Escrow Amount serve as a recourse for the Buyer in case of breach of representations by the Company?\"\n",
    "    \n",
    "    # Generate answer with query expansion and RAG\n",
    "    output = rag_with_query_expansion(original_query)\n",
    "   \n",
    "    \n",
    "    # Print or use the answer as needed\n",
    "    print(\"Generated Answer:\")\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
