{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-11/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from chromadb import Client\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import openai\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = Client()\n",
    "\n",
    "def preprocess_document(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    docx_texts = [paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
    "\n",
    "    character_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], chunk_size=500, chunk_overlap=0)\n",
    "    character_split_texts = character_splitter.split_text('\\n\\n'.join(docx_texts))\n",
    "\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "    token_split_texts = []\n",
    "    for text in character_split_texts:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "    return token_split_texts\n",
    "\n",
    "# Example document path\n",
    "docx_path = \"../Evaluation Sets/Raptor Contract.docx\"\n",
    "documents = preprocess_document(docx_path)\n",
    "\n",
    "# Embedding function\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "# Create ChromaDB collection and add documents\n",
    "chroma_collection = chroma_client.create_collection(\"Contract\", embedding_function=embedding_function)\n",
    "ids = [str(i) for i in range(len(documents))]\n",
    "chroma_collection.add(ids=ids, documents=documents)\n",
    "\n",
    "def query_documents(query, collection):\n",
    "    results = collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results['documents'][0]\n",
    "    return retrieved_documents\n",
    "\n",
    "def augment_multiple_query(query, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert contract advisor assistant. Your users are asking questions about information contained in the contract.\"\n",
    "                       \"Suggest up to five additional related questions to help them find the information they need, for the provided question. \"\n",
    "                       \"Suggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topic.\"\n",
    "                       \"Make sure they are complete questions, and that they are related to the original question.\"\n",
    "                       \"Output one question per line. Do not number the questions.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message['content']\n",
    "    content = content.split(\"\\n\")\n",
    "    return content\n",
    "\n",
    "def rag_with_query_expansion(original_query, model=\"gpt-3.5-turbo\"):\n",
    "    # Expand the original query\n",
    "    expanded_queries = augment_multiple_query(original_query, model=model)[:5]  # Limit to top 5 expanded queries\n",
    "\n",
    "    # Include the original query in the list of queries\n",
    "    queries = [original_query] + expanded_queries\n",
    "\n",
    "    # Query ChromaDB for relevant documents\n",
    "    results = chroma_collection.query(query_texts=queries, n_results=5, include=['documents'])\n",
    "    retrieved_documents = results['documents']\n",
    "\n",
    "    # Deduplicate the retrieved documents\n",
    "    unique_documents = set()\n",
    "    for documents in retrieved_documents:\n",
    "        for document in documents:\n",
    "            unique_documents.add(document)\n",
    "\n",
    "    # Convert back to list for indexing\n",
    "    unique_documents = list(unique_documents)\n",
    "\n",
    "    # Rank documents based on relevance to each query\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    pairs = [[query, doc] for doc in unique_documents for query in queries]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    # Sort documents based on average score across all queries\n",
    "    avg_scores = np.mean(np.array(scores).reshape(len(queries), -1), axis=0)\n",
    "    ranked_indices = np.argsort(avg_scores)[::-1]\n",
    "    ranked_documents = [unique_documents[i] for i in ranked_indices]\n",
    "\n",
    "    # Generate answer using RAG with ranked documents\n",
    "    information = \"\\n\\n\".join(ranked_documents[:5])  # Use top 5 ranked documents\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert legal contract advisor assistant. Your users are asking questions about information contained in the contract.\"\n",
    "                       \"You will be shown the user's question, and the relevant information from the contract. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {original_query}. \\n Information: {information}\"},\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Provide concise answers without additional explanation unless explicitly requested.\"\n",
    "                    \"Identify questions that require a simple yes or no response, and Provide concise answers without additional explanation unless explicitly requested.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message['content']\n",
    "    return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
